{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import autograd, optim, nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from random import shuffle\n",
    "from collections import Counter\n",
    "import argparse\n",
    "\n",
    "import io\n",
    "import gensim\n",
    "#from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n"
     ]
    }
   ],
   "source": [
    "# Load the corpus\n",
    "part = 'full'\n",
    "\n",
    "print(\"loading...\")\n",
    "if part==\"part\":\n",
    "    text = open('rt-polaritydata/rt-polaritydata/rt-polarity.pos', mode='r', encoding='utf-16').readlines()[0][:10000] #Load a part of corpus for debugging\n",
    "elif part==\"full\":\n",
    "    neg_text = open('rt-polaritydata/rt-polaritydata/rt-polarity.neg',mode='r', encoding='utf-16').readlines()\n",
    "    pos_text = open('rt-polaritydata/rt-polaritydata/rt-polarity.pos',mode='r', encoding='utf-16').readlines()\n",
    "else:\n",
    "    print(\"Unknown argument : \" + part)\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "Done. total_sentences: 10662, total_words: 224067, vocab: 21416\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "print(\"preprocessing...\")\n",
    "\n",
    "corpus=[]\n",
    "for sentence in neg_text:\n",
    "    corpus.append((sentence.rstrip(), 0))\n",
    "for sentence in pos_text:\n",
    "    corpus.append((sentence.rstrip(), 1))\n",
    "\n",
    "# Create Vocabulary Dicts\n",
    "words=[]\n",
    "text = neg_text + pos_text\n",
    "for sentence in text:\n",
    "    words = words + sentence.split()\n",
    "    \n",
    "vocab = set(words)\n",
    "w2i = {}\n",
    "w2i[' '] = 0\n",
    "i = 1\n",
    "for word in vocab:\n",
    "    w2i[word] = i\n",
    "    i += 1   \n",
    "i2w = {}\n",
    "for k, v in w2i.items():\n",
    "    i2w[v] = k\n",
    "    \n",
    "corpus_size = len(corpus)\n",
    "vocab_size = len(vocab)\n",
    "words_size = len(words)\n",
    "print(\"Done. total_sentences: %d, total_words: %d, vocab: %d\" %(corpus_size, words_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividing sets done. Train_sentences: 9596, Test_sentences: 1066\n"
     ]
    }
   ],
   "source": [
    "# Divide Training set & Test set\n",
    "def divide(x, train_prop):\n",
    "    random.shuffle(x)\n",
    "    x_train = x[:round(train_prop * len(x))]\n",
    "    x_test = x[round(train_prop * len(x)):]\n",
    "    return x_train, x_test\n",
    "\n",
    "corpus_train, corpus_test = divide(corpus, 0.9)\n",
    "train_size = len(corpus_train)\n",
    "test_size = len(corpus_test)\n",
    "print(\"Dividing sets done. Train_sentences: %d, Test_sentences: %d\" %(train_size, test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'pretrained'\n",
    "def create_set(corpus, vocab, w2i, mode):\n",
    "    set = []\n",
    "    for i, (sentence, label) in enumerate(corpus):\n",
    "        activated = []\n",
    "        words = sentence.split()\n",
    "        if mode == 'rand':\n",
    "            for word in words:\n",
    "                if word not in vocab:\n",
    "                    continue\n",
    "                activated.append(w2i[word])\n",
    "            set.append((activated, label))\n",
    "        elif mode == 'pretrained':\n",
    "            set.append((words, label))\n",
    "    return set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 9596 size of train_set and 1066 of test_set.\n"
     ]
    }
   ],
   "source": [
    "# 3. Create Training set & Test set\n",
    "corpus_train, corpus_test = divide(corpus, 0.9)\n",
    "train_set = create_set(corpus_train, vocab, w2i, mode)\n",
    "test_set = create_set(corpus_test, vocab, w2i, mode)\n",
    "print(\"Created %d size of train_set and %d of test_set.\" % (len(train_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['since',\n",
       "   'lee',\n",
       "   'is',\n",
       "   'a',\n",
       "   'sentimentalist',\n",
       "   ',',\n",
       "   'the',\n",
       "   'film',\n",
       "   'is',\n",
       "   'more',\n",
       "   'worshipful',\n",
       "   'than',\n",
       "   'your',\n",
       "   'random',\n",
       "   'e',\n",
       "   '!',\n",
       "   'true',\n",
       "   'hollywood',\n",
       "   'story',\n",
       "   '.'],\n",
       "  0),\n",
       " (['an',\n",
       "   'emotionally',\n",
       "   'and',\n",
       "   'spiritually',\n",
       "   'compelling',\n",
       "   'journey',\n",
       "   'seen',\n",
       "   'through',\n",
       "   'the',\n",
       "   'right',\n",
       "   'eyes',\n",
       "   ',',\n",
       "   'with',\n",
       "   'the',\n",
       "   'right',\n",
       "   'actors',\n",
       "   'and',\n",
       "   'with',\n",
       "   'the',\n",
       "   'kind',\n",
       "   'of',\n",
       "   'visual',\n",
       "   'flair',\n",
       "   'that',\n",
       "   'shows',\n",
       "   'what',\n",
       "   'great',\n",
       "   'cinema',\n",
       "   'can',\n",
       "   'really',\n",
       "   'do',\n",
       "   '.'],\n",
       "  1),\n",
       " (['might', 'best', 'be', 'enjoyed', 'as', 'a', 'daytime', 'soaper', '.'], 0),\n",
       " (['naipaul',\n",
       "   'fans',\n",
       "   'may',\n",
       "   'be',\n",
       "   'disappointed',\n",
       "   '.',\n",
       "   'those',\n",
       "   'who',\n",
       "   'are',\n",
       "   'not',\n",
       "   'acquainted',\n",
       "   'with',\n",
       "   'the',\n",
       "   \"author's\",\n",
       "   'work',\n",
       "   ',',\n",
       "   'on',\n",
       "   'the',\n",
       "   'other',\n",
       "   'hand',\n",
       "   ',',\n",
       "   'may',\n",
       "   'fall',\n",
       "   'fast',\n",
       "   'asleep',\n",
       "   '.'],\n",
       "  0),\n",
       " (['\"',\n",
       "   'brown',\n",
       "   'sugar',\n",
       "   '\"',\n",
       "   'admirably',\n",
       "   'aspires',\n",
       "   'to',\n",
       "   'be',\n",
       "   'more',\n",
       "   'than',\n",
       "   'another',\n",
       "   '\"',\n",
       "   'best',\n",
       "   'man',\n",
       "   '\"',\n",
       "   'clone',\n",
       "   'by',\n",
       "   'weaving',\n",
       "   'a',\n",
       "   'theme',\n",
       "   'throughout',\n",
       "   'this',\n",
       "   'funny',\n",
       "   'film',\n",
       "   '.'],\n",
       "  1),\n",
       " (['i',\n",
       "   'had',\n",
       "   'more',\n",
       "   'fun',\n",
       "   'watching',\n",
       "   'spy',\n",
       "   'than',\n",
       "   'i',\n",
       "   'had',\n",
       "   'with',\n",
       "   'most',\n",
       "   'of',\n",
       "   'the',\n",
       "   'big',\n",
       "   'summer',\n",
       "   'movies',\n",
       "   '.'],\n",
       "  1),\n",
       " ([\"shreve's\",\n",
       "   'graceful',\n",
       "   'dual',\n",
       "   'narrative',\n",
       "   'gets',\n",
       "   'clunky',\n",
       "   'on',\n",
       "   'the',\n",
       "   'screen',\n",
       "   ',',\n",
       "   'and',\n",
       "   'we',\n",
       "   'keep',\n",
       "   'getting',\n",
       "   'torn',\n",
       "   'away',\n",
       "   'from',\n",
       "   'the',\n",
       "   'compelling',\n",
       "   'historical',\n",
       "   'tale',\n",
       "   'to',\n",
       "   'a',\n",
       "   'less-compelling',\n",
       "   'soap',\n",
       "   'opera',\n",
       "   '.'],\n",
       "  0),\n",
       " (['a',\n",
       "   'characteristically',\n",
       "   'engorged',\n",
       "   'and',\n",
       "   'sloppy',\n",
       "   'coming-of-age',\n",
       "   'movie',\n",
       "   '.'],\n",
       "  0),\n",
       " (['the',\n",
       "   'pace',\n",
       "   'of',\n",
       "   'the',\n",
       "   'film',\n",
       "   'is',\n",
       "   'very',\n",
       "   'slow',\n",
       "   '(',\n",
       "   'for',\n",
       "   'obvious',\n",
       "   'reasons',\n",
       "   ')',\n",
       "   'and',\n",
       "   'that',\n",
       "   'too',\n",
       "   'becomes',\n",
       "   'off-putting',\n",
       "   '.'],\n",
       "  0),\n",
       " (['a',\n",
       "   'deftly',\n",
       "   'entertaining',\n",
       "   'film',\n",
       "   ',',\n",
       "   'smartly',\n",
       "   'played',\n",
       "   'and',\n",
       "   'smartly',\n",
       "   'directed',\n",
       "   '.'],\n",
       "  1)]"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 12, in zmq.backend.cython.checkrc._check_rc\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-699-7c6612799158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create PRETRAINED embedding model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpretrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pretrained_word2vec/GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1492\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1493\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \u001b[0;31m# TODO use frombuffer or something similar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline_no\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36madd_word\u001b[0;34m(word, weights)\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vocabulary file is incomplete: '%s' is missing\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create PRETRAINED embedding model\n",
    "pretrained_model = gensim.models.KeyedVectors.load_word2vec_format('pretrained_word2vec/GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model['school']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.random.normal(0, 0.1, 300)\n",
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.add('schood', [s], replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-625-491cb7bd893a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'schood'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetflags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "pretrained_model['schood']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextCNN neural network \n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, mode, num_filter = 100, window_sizes=(3, 4, 5)):\n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        if mode == 'rand':\n",
    "            # Initialize random embeddings \n",
    "            self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        elif mode == 'pretrained':\n",
    "            self.embedding = pretrained_model\n",
    "            \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(1, 100, [window_size, emb_size], padding=(window_size -1, 0)) \n",
    "                                   for window_size in window_sizes])\n",
    "\n",
    "        self.fc = nn.Linear(num_filter * len(window_sizes), 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, mode):\n",
    "        #######################################################################################\n",
    "        # (Matrix Size Info)\n",
    "        # B = batch size\n",
    "        # C = channel dimension\n",
    "        # L = this batch's max sentence length\n",
    "        # E = embedding dimension\n",
    "        #######################################################################################\n",
    "        \n",
    "        if mode == 'rand': # x = activated index\n",
    "            x = torch.LongTensor(x)\n",
    "            x = self.embedding(x)\n",
    "            \n",
    "        elif mode == 'pretrained': # x = words lists\n",
    "            # Tricky partIf a word is OOV, should be initialized first.\n",
    "            xs = []\n",
    "            for batch_element in x:\n",
    "                xw = []\n",
    "                for word in batch_element:\n",
    "                    if word in self.embedding:\n",
    "                        xw.append(self.embedding[word])\n",
    "                    else:\n",
    "                        rand_vec = np.random.normal(0, 0.1, 300)\n",
    "                        xw.append(rand_vec)\n",
    "                xs.append(xw)\n",
    "            x = torch.DoubleTensor(xs)\n",
    "            \n",
    "        else:\n",
    "            print(\"Unknown mode. Terminated\")\n",
    "            return mode\n",
    "        \n",
    "        x = torch.unsqueeze(x, 1)             # [B, C, L, E] Add a channel dim.\n",
    "        temp = []\n",
    "        for conv in self.convs:\n",
    "            x2 = self.relu(conv(x))           # [B, F, L, 1]\n",
    "            x2 = torch.squeeze(x2, -1)        # [B, F, L]\n",
    "            x2 = F.max_pool1d(x2, x2.size(2)) # [B, F, 1]\n",
    "            temp.append(x2)\n",
    "        x = torch.cat(temp, 2)                # [B, F, window]\n",
    "\n",
    "        # Drop & FC\n",
    "        x = self.drop(x)\n",
    "        flatten = x.view(x.size(0), -1)       # [B, F * window]\n",
    "        logits = self.fc(flatten)             # [B, class]\n",
    "\n",
    "        # Regularization\n",
    "        norm = model.fc.weight.norm()\n",
    "        if norm > 3:\n",
    "            rescaled = model.fc.weight * 3 / norm\n",
    "            model.fc.weight = nn.Parameter(rescaled)\n",
    "        \n",
    "        # Prediction\n",
    "        probs = F.softmax(logits)       # [B, class]\n",
    "        classes = torch.max(probs, 1)[1]# [B]\n",
    "\n",
    "        return probs, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextCNN(vocab_size, 300, mode)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter\n",
    "num_epochs = 5\n",
    "num_batch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "emb = rand_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainer(train_set, num_epochs, num_batch, model, optimizer, loss_function, mode):\n",
    "    print(\"Start training with %d num of train data\" %(len(train_set)))\n",
    "    total_step = int(len(train_set) / num_batch)\n",
    "    for epoch in range(num_epochs):\n",
    "        end = 0\n",
    "        for step in range(total_step):\n",
    "            # Batch organization\n",
    "            start = end\n",
    "            end = start + num_batch\n",
    "            wordlists, labels = list(zip(*train_set[start:end]))\n",
    "            wordlists = list(wordlists)\n",
    "\n",
    "            # Add padding to resize the data\n",
    "            max_len = 0\n",
    "            for wordlist in wordlists:\n",
    "                temp = len(wordlist)\n",
    "                if temp > max_len:\n",
    "                    max_len = temp  # max length for 'this' batch\n",
    "            for i in range(num_batch):\n",
    "                if mode == 'rand':\n",
    "                    wordlists[i] = wordlists[i] + [0] * (max_len - len(wordlists[i]))\n",
    "                elif mode == 'pretrained':\n",
    "                    wordlists[i] = wordlists[i] + [' '] * (max_len - len(wordlists[i]))\n",
    "                    \n",
    "                    \n",
    "            labels = torch.tensor(labels)\n",
    "            if mode == 'rand': # x = activated index\n",
    "            x = torch.LongTensor(x)\n",
    "            x = self.embedding(x)\n",
    "            \n",
    "            elif mode == 'pretrained': # x = words lists\n",
    "                # Tricky partIf a word is OOV, should be initialized first.\n",
    "                xs = []\n",
    "                for batch_element in x:\n",
    "                    xw = []\n",
    "                    for word in batch_element:\n",
    "                        if word in self.embedding:\n",
    "                            xw.append(self.embedding[word])\n",
    "                        else:\n",
    "                            rand_vec = np.random.normal(0, 0.1, 300)\n",
    "                            xw.append(rand_vec)\n",
    "                    xs.append(xw)\n",
    "                x = torch.DoubleTensor(xs)\n",
    "            \n",
    "            else:\n",
    "                print(\"Unknown mode. Terminated\")\n",
    "                return mode\n",
    "\n",
    "            # Forward pass\n",
    "            probs, classes = model(wordlists, mode, train)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            losses = loss_function(probs, labels)\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (step + 1) % 10 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, step + 1, total_step,\n",
    "                                                                         losses.item()))\n",
    "\n",
    "    print(\"Training Done.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training with 9596 num of train data\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-766-4977233e1f71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-765-fa3e87405475>\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(train_set, num_epochs, num_batch, model, optimizer, loss_function, mode)\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mxw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_element\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                             \u001b[0mxw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# 4. Train model\n",
    "num_epochs = 5\n",
    "num_batch = 50\n",
    "\n",
    "model = TextCNN(vocab_size, 300, mode)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "trained_model = trainer(train_set, num_epochs, num_batch, model, optimizer, loss_function, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['the',\n",
       "  'band',\n",
       "  'performances',\n",
       "  'featured',\n",
       "  'in',\n",
       "  'drumline',\n",
       "  'are',\n",
       "  'red',\n",
       "  'hot',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '[but]',\n",
       "  'from',\n",
       "  'a',\n",
       "  'mere',\n",
       "  'story',\n",
       "  'point',\n",
       "  'of',\n",
       "  'view',\n",
       "  ',',\n",
       "  'the',\n",
       "  \"film's\",\n",
       "  'ice',\n",
       "  'cold',\n",
       "  '.'],\n",
       " 0)"
      ]
     },
     "execution_count": 745,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "batch\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 1 (got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-760-8eb9adb13003>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mcorrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-755-e077eb0a1ca7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mode)\u001b[0m\n\u001b[1;32m     52\u001b[0m                         \u001b[0mrand_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                         \u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoubleTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 3 at dim 1 (got 4)"
     ]
    }
   ],
   "source": [
    "# Predict test set\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    corrects = 0\n",
    "    for wordlist, label in test_set:\n",
    "        probs, classes = model(wordlist, mode)\n",
    "        if classes.item() == label:\n",
    "            corrects += 1\n",
    "\n",
    "    print(\"Accuracy: \", corrects/test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5850, 0.4150]], grad_fn=<SoftmaxBackward>) tensor([0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.LongTensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "probs, classes = model(inputs.unsqueeze(0))\n",
    "print(probs, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tester(train_set, model, optimizer, loss_function, mode):\n",
    "    num_epochs=1\n",
    "    num_batch=10\n",
    "    test_size = len(test_set)\n",
    "    print(\"Start test with %d num of test data\" %(test_size))\n",
    "    total_step = int(test_size / num_batch)\n",
    "    corrects = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        end = 0\n",
    "        for step in range(total_step):\n",
    "            # Batch organization\n",
    "            start = end\n",
    "            end = start + num_batch\n",
    "            wordlists, labels = list(zip(*test_set[start:end]))\n",
    "            wordlists = list(wordlists)\n",
    "\n",
    "            # Add padding to resize the data\n",
    "            max_len = 0\n",
    "            for wordlist in wordlists:\n",
    "                temp = len(wordlist)\n",
    "                if temp > max_len:\n",
    "                    max_len = temp  # max length for 'this' batch\n",
    "            for i in range(num_batch):\n",
    "                if mode == 'rand':\n",
    "                    wordlists[i] = wordlists[i] + [0] * (max_len - len(wordlists[i]))\n",
    "                elif mode == 'pretrained':\n",
    "                    wordlists[i] = wordlists[i] + [' '] * (max_len - len(wordlists[i]))\n",
    "\n",
    "            # Forward pass\n",
    "            probs, classes = model(wordlists, mode)\n",
    "            \n",
    "            classes = classes.tolist()\n",
    "            \n",
    "            for i in range(num_batch):\n",
    "                if classes[i] == labels[i]:\n",
    "                    corrects += 1\n",
    "\n",
    "    print(\"Accuracy: \", corrects/test_size)\n",
    "    return corrects/test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start test with 1066 num of test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4906191369606004\n"
     ]
    }
   ],
   "source": [
    "acuuracy = tester(test_set, model, optimizer, loss_function, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
